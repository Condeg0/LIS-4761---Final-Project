---
title: "Final Project"
author: "Rafael Cond√©"
date: "`r Sys.Date()`"
output: html_document
---

===== Load libraries =====
```{r}
library(quantmod)
library(dplyr)
library(lubridate)
library(tm)
library(slam)
library(wordcloud)
library(syuzhet)
library(ggplot2)
library(lmtest)
library(ggcorrplot)
```


===== Load & Read Data =====
```{r}
# Get tweet data 
tweet_data <- read.csv("Data/Nvidia-Tweets.csv")
tweet_data <- na.omit(tweet_data)  # Remove NA values

# Remove duplicate tweets based on text
tweet_data <- tweet_data %>%
  distinct(Text, .keep_all = TRUE)

# Format date column
tweet_data$Day <- as.Date(tweet_data$Datetime)
tweet_data$Time <- format(as.POSIXct(tweet_data$Datetime), "%H:%M:%S")
tweet_data$Hour <- hour(ymd_hms(tweet_data$Datetime))


# Get stock price data 
start <- min(tweet_data$Day)
end <- max(tweet_data$Day)
nvda_data <- getSymbols("NVDA", from = start, to = end, auto.assign = F)

nvda_data$LogReturn <- dailyReturn(nvda_data, type = "log")
nvda_data$AReturn <- dailyReturn(nvda_data, type = "arithmetic")
nvda_data <- as.data.frame(nvda_data)
nvda_data$Date <- as.Date(rownames(nvda_data))

```



===== Text Analysis/ Mining ====

Text pre-processing
```{r}
 # Convert the data to appropriate format
corpus <- VCorpus(VectorSource(tweet_data[1:6000, ]$Text))

# Clean data
stop_words <- stopwords("en")
corpus <- tm_map(corpus, content_transformer(function(x) removeWords(x, stop_words)))  # Remove stop words
corpus <- tm_map(corpus, content_transformer(tolower)) # Lowercase
corpus <- tm_map(corpus, removeNumbers)                # Remove numbers
corpus <- tm_map(corpus, removePunctuation)            # Remove punctuation
corpus <- tm_map(corpus, stripWhitespace)              # Remove extra whitespace
```

Explore corpus
```{r}
tdm <- TermDocumentMatrix(corpus)
tdm_matrix <- as.matrix(tdm)
word_freq <- rowSums(tdm_matrix)
word_freq <- sort(word_freq, decreasing = TRUE)

# Wordcloud
#wordcloud(names(word_freq), word_freq, max.words = 100)
```

Sentiment Analysis
```{r}
# Give a score to each tweet
tweet_data$Sentiment <- get_sentiment(tweet_data$Text, method = "afinn")

# Get sentiment score and tweet volume
df <- tweet_data %>%
  mutate(SentimentType = case_when(
    Sentiment > 0 ~ "positive",
    Sentiment < 0 ~ "negative",
    TRUE ~ "neutral"
  )) %>%
  group_by(Day) %>%
  summarize(
    MeanSentiment = mean(Sentiment),
    TweetVolume = n(),
    Positive = sum(SentimentType == "positive"),
    Negative = sum(SentimentType == "negative"),
    Neutral = sum(SentimentType == "neutral"),
    NegPosRatio = ifelse(Positive > 0, Negative / Positive, NA),
    PosRatio = ifelse(Positive > 0, Positive / TweetVolume, NA),
    NegRatio = ifelse(Negative > 0, Negative / TweetVolume, NA)) 

# Merge text mining data and stock data
merged_data <- merge(x = nvda_data, y = df, by.x = "Date", by.y = "Day", all.y = T)

merged_data <- merged_data %>%
  mutate(
    LaggedSentiment = lag(MeanSentiment, 1),
    Volatility = abs(AReturn),  # absolute return as proxy
    LaggedAReturn = lag(AReturn, 1),
    LaggedVolatility = lag(Volatility, 1),
    LaggedNVDA_Volume = lag(NVDA.Volume, 1),
    LaggedTVolume = lag(TweetVolume, 1)
  )

# Select only numeric columns you care about
cor_data <- merged_data %>%
  select(
    AReturn, Volatility, MeanSentiment, LaggedSentiment, TweetVolume,
    LaggedTVolume, NVDA.Volume, LaggedNVDA_Volume, LaggedAReturn,
    LaggedVolatility) %>%
  na.omit()


# Compute correlation matrix
cor_matrix <- cor(cor_data)

write.csv(x = merged_data, file = "Data/merged_data.csv")

```

Visualization
```{r}
# Plots 
# Lagged Mean sentiment and Return
ggplot(data = na.omit(merged_data), aes(x = Date)) +
  geom_line(aes(y = LaggedSentiment)) +
  geom_line(aes(y = AReturn))

# Tweet Volume and Trading volume
ggplot(data = na.omit(merged_data), aes(x = Date)) +
  geom_line(aes(y = TweetVolume)) +
  geom_line(aes(y = NVDA.Volume))

# Visualize correlations
ggcorrplot(cor_matrix, 
           method = "circle", 
           type = "lower", 
           lab = TRUE, 
           tl.cex = 10,
           colors = c("darkred", "white", "darkblue"),
           title = "Correlation Matrix of Sentiment, Volume & Returns")


# Linear models
# Return and lagged mean sentiment
lm_model <- lm(formula = AReturn ~ LaggedSentiment, data = merged_data)
summary(lm_model)
plot(lm_model)

# Trading volume and Tweet Volume
lm_model_volume <- lm(formula = NVDA.Volume ~ TweetVolume, data = merged_data)
summary(lm_model_volume)
plot(lm_model_volume)

```
