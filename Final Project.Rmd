---
title: "Final Project Deliverable"
author: "Anthony Nance, Rafael Cond√© Gomes, Matthew Humphreys, Travis Race, James Angeles"
date: "`r Sys.Date()`"
output: html_document
---


# Load libraries
```{r setup, include=FALSE}
library(quantmod)  # For stock data
library(dplyr)  # For data & text processing/ cleaning
library(lubridate)  # For dates
library(tm)  # Text processing
library(slam)
library(wordcloud)  # Visualization
library(syuzhet)
library(ggplot2)  # Visualization
library(lmtest)  # Statistical tests
library(ggcorrplot)  # Visualizatoin
library(quanteda)  # Text processing (more efficient)
library(e1071)  # SVM
library(caret)  # SVM
library(GGally) # Exploratory Analysis
library(zoo) # Exploratory Analysis
library(lubridate) # Exploratory Analysis
library(forecast) # Exploratory Analysis
```


# Load & Read Data
```{r}
# Get tweet data 
tweet_data <- read.csv("Data/Nvidia-Tweets.csv")
tweet_data <- na.omit(tweet_data)  # Remove NA values

# Remove duplicate tweets based on text
tweet_data <- tweet_data %>%
  distinct(Text, .keep_all = TRUE)

# Format date column
tweet_data$Day <- as.Date(tweet_data$Datetime)
tweet_data$Time <- format(as.POSIXct(tweet_data$Datetime), "%H:%M:%S")
tweet_data$Hour <- hour(ymd_hms(tweet_data$Datetime))


# Get stock price data 
start <- min(tweet_data$Day)
end <- max(tweet_data$Day)
nvda_data <- getSymbols("NVDA", from = start, to = end, auto.assign = F)

nvda_data$LogReturn <- dailyReturn(nvda_data, type = "log")
nvda_data$AReturn <- dailyReturn(nvda_data, type = "arithmetic")
nvda_data <- as.data.frame(nvda_data)
nvda_data$Date <- as.Date(rownames(nvda_data))

```



# Text Analysis/ Mining

## Text processing & text mining
```{r}
# Remove unecessary columns from tweet data
tweet_data <- tweet_data[ , c("Text", "Day")]

# Create corpus and tokens
corpus <- corpus(tweet_data$Text)
tokens <- tokens(corpus, remove_punct = TRUE, remove_symbols = TRUE)
tokens <- tokens_tolower(tokens)
tokens <- tokens_remove(tokens, stopwords("en"))

# Create document-feature matrix (DFM)
# For performance's sake
dfm <- dfm(tokens)
word_freq <- data.frame(topfeatures(dfm, 1000))

clean_text <- sapply(tokens, function(x) paste(x, collapse = " "))
tweet_data$CleanText <- clean_text
```

## Sentiment Analysis
```{r}
# Give a score to each tweet
tweet_data$Sentiment <- get_sentiment(tweet_data$CleanText, method = "afinn")

# Get sentiment score and tweet volume
df <- tweet_data %>%
  mutate(SentimentType = case_when(
    Sentiment > 0 ~ "positive",
    Sentiment < 0 ~ "negative",
    TRUE ~ "neutral"
  )) %>%
  group_by(Day) %>%
  summarize(
    MeanSentiment = mean(Sentiment),
    TweetVolume = n(),
    Positive = sum(SentimentType == "positive"),
    Negative = sum(SentimentType == "negative"),
    Neutral = sum(SentimentType == "neutral"),
    NegPosRatio = ifelse(Positive > 0, Negative / Positive, NA),
    PosRatio = ifelse(Positive > 0, Positive / TweetVolume, NA),
    NegRatio = ifelse(Negative > 0, Negative / TweetVolume, NA)) 


# Create target variable: direction of next-day closing price
# For SVM analysis
nvda_data <- nvda_data %>%
  mutate(Close_lead = lead(NVDA.Close),
         Direction = if_else(Close_lead > NVDA.Close, "Up", "Down")) %>%
  na.omit()


# Merge text mining data and stock data
merged_data <- merge(x = nvda_data, y = df, by.x = "Date", by.y = "Day", all.y = T)

merged_data <- merged_data %>%
  mutate(
    LaggedSentiment = lag(MeanSentiment, 1),
    Volatility = abs(AReturn),  # absolute return as proxy
    LaggedAReturn = lag(AReturn, 1),
    LaggedVolatility = lag(Volatility, 1),
    LaggedNVDA_Volume = lag(NVDA.Volume, 1),
    LaggedTVolume = lag(TweetVolume, 1)
  )

# Select only numeric columns you care about
cor_data <- merged_data %>%
  select(
    AReturn, Volatility, MeanSentiment, LaggedSentiment, TweetVolume,
    LaggedTVolume, NVDA.Volume, LaggedNVDA_Volume, LaggedAReturn,
    LaggedVolatility) %>%
  na.omit()


# Compute correlation matrix
cor_matrix <- cor(cor_data)

write.csv(x = merged_data, file = "Data/merged_data.csv")

```

Visualization
```{r}
# Plots 
# Lagged Mean sentiment and Return
ggplot(data = na.omit(merged_data), aes(x = Date)) +
  geom_line(aes(y = LaggedSentiment, color = "Lagged Mean Sentiment")) +
  geom_line(aes(y = AReturn, color = "Return")) +
  labs(title = "Lagged Sentiment & Return", x = "Date", y = "") +
  scale_color_manual(values = c("Return" = "darkblue", "Lagged Mean Sentiment" = "darkred"), name = "")

 
# Visualize correlations
ggcorrplot(cor_matrix, 
           method = "circle", 
           type = "lower", 
           lab = TRUE, 
           tl.cex = 10,
           colors = c("darkred", "white", "darkblue"),
           title = "Correlation Matrix of Sentiment, Volume & Returns")

# Linear models
# Return and lagged mean sentiment
lm_model <- lm(formula = AReturn ~ LaggedSentiment, data = merged_data)
summary(lm_model)
plot(lm_model)

# Trading volume and Tweet Volume
lm_model_volume <- lm(formula = NVDA.Volume ~ TweetVolume, data = merged_data)
summary(lm_model_volume)
plot(lm_model_volume)

```


===== Exploratory Analysis =====
```{r}

```


===== SVM =====
```{r}

```



===== Shiny App =====
```{r}

```

